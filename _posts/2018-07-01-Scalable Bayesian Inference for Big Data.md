---
layout: post
title:  "Scalable Bayesian Inference for Big Data"
date:   2019-06-31 12:00:00 +0800
categories: Big-Data Bayesian Inference
comments: true
use_math : true
author: Al-Ahmadgaid B. Asaad
tag: julia
---
In Statistics, inference is a process of characterizing the population from a given data. There are two approaches to inference, namely <b>Frequentist</b> and <b>Bayesian</b> approaches. The former is based on the assumption that, the objective parameter of the model (e.g. density curve for the population distribution) given the data, is fixed and unknown. On the other hand, the latter (Bayesian inference) describes this parameter as random (following some distribution) and unknown. Having this understanding then, suggests that Bayesian inference somehow extends the Frequentist solution by giving out multiple parameter estimates that behave according to some derived distribution --- referred to as the posterior distribution or <b>a posteriori</b> --- as opposed to single point estimate on the objective parameters. Therefore, of the two approaches, the Frequentist procedure to inference is more popular in terms of the availability of tools, ranging from handling small data to big data. Big data as <a href="https://www.ibmbigdatahub.com/infographic/four-vs-big-data">defined by the International Business Machines (IBM)</a>, can refer to data that are either big in <b>volume</b>, real-time in <b>velocity</b>, varied in <b>variety</b>, and dirty in <b>veracity</b>. The last two attributes (<b>variety</b> and <b>veracity</b>) mentioned are well-studied in Statistics, but not the first two attributes. Hence, with the advent of big data, recent theoretical developments now include statistics for analyzing data that are big in both <b>volume</b> and <b>velocity</b>. For this paper, however, the focus will be on handling the <b>volume</b>; but needless to say, there are interesting research for handling streaming data as well, one of it is called <b>online statistics</b> [1].

Data that are big in <b>volume</b> can either refer to big in <b>n</b> or <b>tall data</b>, which means the number of records is big (e.g. in the range of millions, billions and more); or big in <b>p</b> also referred to as <b>wide data</b>, in which the number of variables or features not only is bigger than the number of records or rows, but also large enough that it could also go millions and more. In theory, Bayesian is well-known to be good at handling small data in comparison to Frequentist, thanks to the prior specification. However, when it comes to big data, the availability of stable software leaned towards the Frequentist algorithms. A good example of this is the Machine Learning (ML) library of the <a href="https://spark.apache.org/">Apache Spark</a> --- a software developed for handling big data (mostly big <b>n</b>). A typical example of big in <b>n</b> data is the transaction log of store front business, where records goes Gigabytes to Terabytes or more in size, but are often stored in relational table. On the other hand, wide data or data that are big in <b>p</b> often are unstructured in form, for example images and videos. This follows from the fact that, images are represented by its pixels and channels (e.g. RGB --- Red, Green, and Blue channels) of each pixel. Hence, vectorizing these pixels across height ($h$), width ($w$) and channels ($\lambda$) of an image, generates a $h\times w\times\lambda$ dimensions. In matrix format, this translate to saying, each row represents an image, and each image (row) has corresponding $h\times w\times\lambda$ columns/variables that describe this image. So for a 500$\times$500 colored image with 3 channels (RGB), if vectorized, can have 750,000 dimensions. For this paper, we refer to images as a type of big data since it is unstructured as well. Having said, numerous models have been proposed including the complex ones for handling images, these models are called <b>Deep Learning</b> (DL) models. In fact, the success of AI nowadays are due to these models, most of which are not Bayesian in nature. The success is also due to the availability of software that can do <b>automatic differentiation</b> on any specified models. As such, derivation of complex loss functions is not anymore a problem of doing it manually, since computers can do it quite well already. Thus, coming up with a new architecture or a new variant of <b>Deep Neural Networks</b> or any other complex models is now relatively easy to implement and train, thanks to high-level Application Program Interfaces (APIs) of tools like <b>Google's Tensorflow</b> and <b>Torch</b> projects. Therefore, the success of AI recently in terms of computer vision and natural language processing as stressed above, is partly due to the availability of the stable tools, of which the estimation is based on the Frequentist's assumption of the weights.

While recent computer visions and natural language processing techniques gained significant improvements on the predictive capability under the Frequentist approach, this type of inference, as mentioned earlier is based on the assumption that the objective parameter is fixed. Having this nature, however, somehow limits the researcher on specifying other known characteristics of the model that could further contribute to the solutions of the problem. As an illustration, consider the Michaelis-Menten model below:
\begin{equation}\label{eq:mm}
    y = \frac{\beta_0 x}{\beta_1 + x}.
\end{equation}
This equation is popular for modeling enzyme kinetics, and the goal is to find the optimal value for $\beta_0$ and $\beta_1$ given the data. The estimation procedure can be done via <b>mathematical programming</b> since there is no closed form solution to this nonlinear regression. However, this model can be linearly transformed and have a closed form solution. In any case, the user will only get single solution for the values of $\beta_0$ and $\beta_1$, which is not a problem, but wouldn't it be nice to further express what we know about the parameters we are interested in? For example, the two parameters above must be positive in practice. Wouldn't it be useful to specify this? Wouldn't it be predictive if we can specify how they behave? If so, then consider using Bayesian inference --- where instead of minimizing the error of Equation (\ref{eq:mm}) given below:
\begin{equation}\label{eq:mm-error}
    \varepsilon = y - \frac{\beta_0 x}{\beta_1 + x},
\end{equation}
and optimized over the weights, what Bayesian does is to solve this probabilistically by first specifying the problem as follows, for example:

$$
\begin{align}
    \label{eq:iidys}
    y &\sim \mathcal{N}\left(\mu = \frac{\beta_0 x}{\beta_1 + x}, \frac{1}{5}\right)\\
    \label{eq:apriori}
    (\beta_0, \beta_1) &\sim \mathcal{TN}(\mu = 0, \sigma = 100, a = 0, b = \infty),
\end{align}
$$

where $\mathcal{TN}(\cdot)$ is the notation for the Truncated Normal distribution.
After that, estimation is done by deriving the posterior distribution of the objective parameters by simplifying the Bayes' theorem given below:
\begin{equation}\label{eq:bayes-thm-0}
\mathbb{P}(\boldsymbol{\beta}|\mathbf{y}) = \frac{\mathbb{P}(\boldsymbol{\beta})\mathbb{P}(\mathbf{y}|\boldsymbol{\beta})}{\mathbb{P}(\mathbf{y})},
\end{equation}
where $\mathbb{P}(\mathbf{y}|\boldsymbol{\beta})$ is the likelihood, which is the product of Equation (\ref{eq:iidys}), and is described by a Normal density function; $\mathbb{P}(\boldsymbol{\beta})$ is the prior distribution or <b>a priori</b> given by Equation (\ref{eq:apriori}), and is characterized by a Truncated Normal with support between 0 and $\infty$; $\mathbb{P}(\mathbf{y})$ is the normalizing constant obtained by integrating out $\boldsymbol{\beta}$ from the likelihood, i.e. $\mathbb{P}(\mathbf{y})=\int \mathbb{P}(\mathbf{y}|\boldsymbol{\beta})\operatorname{d}\boldsymbol{\beta}$. Therefore, in comparison to Frequentist approach, inference in Bayesian is based fully on probability theory.
In this paper, the author will review the recent advances of Bayesian inference with regards to big data problems, specifically for big <b>n</b> and big <b>p</b>. Along these, is a review on actively developed open-source tools for scaling Bayesian inference, which is computationally referred to as <b>probabilistic programming</b>; and finally a review on the application of Bayesian inference for big data, specifically for computer vision and natural language processing.
### Sequential Bayesian Learning
As an illustration of how Bayesian inference works,  consider the input variable $x$ and a target variable $y$ such that the true function is the simple linear regression with parameters $w_0\triangleq-.3$ and $w_1\triangleq-.5$. Suppose the random values of $x$ are taken from a uniform distribution having domain $[-1,1]$; then the target variable $y=h(x,\mathbf{w})+\varepsilon=w_0+w_1x+\varepsilon$, where $\varepsilon$ is a Gaussian noise having mean $0$ and standard deviation $5$. The goal is to recover the true value of $w_0$ and $w_1$ from the data. Using the <b>a priori</b> defined by a standard Gaussian distribution, the plot of this density is given in the first row, second column of Figure 1. This is the case in which no data point yet is observed. The white diamond point in the contour plot of the prior density is the true value of the parameters that needs to be estimated. The corresponding 20 samples of straight lines with weights sampled from the <b>a priori</b> are plotted in the far right hand side of the first row of the figure. These sampled lines form the so called <b>model uncertainty</b>. The second row depicts the case where first observation is observed. The likelihood of this data point is shown in the left hand side of the row, and using this likelihood multiplied by the <b>a priori</b> in the previous row, and further multiply it with the normalizing constant, returns the <b>a posteriori</b> in the middle column of the second row. The corresponding fitted lines are placed in the right hand side of the row. Following the same procedure for the third row, with likelihood of the first five observations shown in the left hand side of the row, and <b>a priori</b> given by the <b>a posteriori</b> of the second row, returns a more concentrated posterior distribution of the weights with 20 sampled fitted lines in the third column of the third row. The process is repeated until the fourth row consisting of 20 observations. This example is based from \cite{bishop} which illustrates the sequential Bayesian learning to fitting a straight line to a simulated data.
<img src="http://drive.google.com/uc?export=view&id=1o9kg2rtzb77eFSUtGGQmeYLqxWPW3ONL" style="width: 95%">
<center>Figure 1: Sequential Bayesian Learning of Fitting Straight Line.</center>

### References
* J. T. Day, Online Algorithms for Statistics, Ph.D. thesis, 2018.
* Yuret, Deniz (2016). <a href="https://pdfs.semanticscholar.org/28ee/845420b8ba275cf1d695fbf383cc21922fbd.pdf">Knet: beginning deep learning with 100 lines of Julia</a>. 30th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.
* Innes, Mike (2018). <a href="http://joss.theoj.org/papers/10.21105/joss.00602">Flux: Elegant machine learning with Julia</a>. Journal of Open Source Software, 3(25), 602, https://doi.org/10.21105/joss.00602
* Abadi, Martin et al (2016). <a href="https://www.usenix.org/system/files/conference/osdi16/osdi16-abadi.pdf">TensorFlow-Keras: A system for large-scale machine learning</a>. 12th USENIX Symposium on Operating Systems Design and Implementation (OSDI 16). p265--283.
